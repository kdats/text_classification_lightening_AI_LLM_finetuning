{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "160f451a-196b-49bb-bc7b-5a3c34d183b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ! pip install -U bitsandbytes\n",
    "# !pip install -U transformers accelerate peft datasets evaluate huggingface_hub pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ece6af75-5f5d-44d2-995c-65c14c139382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "hf_token = os.environ[\"HF_TOKEN\"]\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "956e1269-38e9-43c6-a059-457a66798e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device: NVIDIA A10G\n",
      "CUDA version: 12.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device:\", torch.cuda.get_device_name(0))\n",
    "print(\"CUDA version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "549a021e-2bea-4881-8db1-35d6131662a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db57c7ac521401ca2a29c570b3da76f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc5225692eb4caba0a84382b03d1bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !pip install -q transformers datasets peft evaluate accelerate bitsandbytes\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(5000))\n",
    "test_dataset = dataset[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # fix padding issue explicitly\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "def preprocess(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess, batched=True).remove_columns(\"text\")\n",
    "test_dataset = test_dataset.map(preprocess, batched=True).remove_columns(\"text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6902af6e-d684-40d7-a33a-0f9d078812af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['World', 'Sports', 'Business', 'Sci/Tech']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = train_dataset.features[\"label\"].names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb550d2-aaae-4b55-971d-72d726bc1bb5",
   "metadata": {},
   "source": [
    "### Instantiation for PEFT and LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57e771d9-4d36-45aa-a3bd-3d18d62fe09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at TinyLlama/TinyLlama-1.1B-Chat-v1.0 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,316,032 || all params: 1,040,836,608 || trainable%: 0.6068\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification#, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "\n",
    " # fully open, no gated access\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=4,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"down_proj\", \"up_proj\"],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d1fbdf-a79c-4112-a2c4-e4adeeecd1a1",
   "metadata": {},
   "source": [
    "### Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3adf8bf8-9f48-41aa-9e7c-59af07f5550a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5969/2178116470.py:34: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 11:42, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.118100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.214700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.129800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.074800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.023100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.017800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.017900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.007600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.004200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.03485668756440282, metrics={'train_runtime': 702.8586, 'train_samples_per_second': 22.764, 'train_steps_per_second': 1.423, 'total_flos': 1.1984493871104e+16, 'train_loss': 0.03485668756440282, 'epoch': 8.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import os\n",
    "# os.environ[\"XLA_USE_BF16\"] = \"1\"\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "tokenizer.pad_token   = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return accuracy_metric.compute(predictions=preds, references=labels)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./distilbert-classifier\",\n",
    "    num_train_epochs=8,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=3e-5,\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    label_names=label_names\n",
    "    # fp16=True,\n",
    "    # bf16=True,\n",
    "    # no bf16, no fp16\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    compute_metrics=compute_metrics\n",
    "\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e874ab5a-67b1-4204-b38e-3d9386525056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 9.7194, 'eval_samples_per_second': 51.443, 'eval_steps_per_second': 3.292, 'epoch': 8.0}\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate(test_dataset)\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2e416c-8265-4797-9819-97c22f277083",
   "metadata": {},
   "source": [
    "### SAve Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d615403a-9664-4143-bb01-5ab01f89dc2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./final_model/tokenizer_config.json',\n",
       " './final_model/special_tokens_map.json',\n",
       " './final_model/tokenizer.json')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dir = \"./final_model\"\n",
    "trainer.save_model(final_dir)\n",
    "tokenizer.save_pretrained(final_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c022b5-52bc-4d1b-b0f4-a95fbfb2f797",
   "metadata": {},
   "source": [
    "### New Text Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c97ed9f1-dd02-4f15-bde9-e761d2b6e788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: NASA launches a new research satellite to study climate change effects.\n",
      "  Predicted label: Sci/Tech\n",
      "\n",
      "Text: The Barcelona lost the last Soccer Match Cricket.\n",
      "  Predicted label: Sports\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Put model in evaluation mode and move to CPU (or GPU) for inference\n",
    "model.eval()\n",
    "model.to(\"cuda\")\n",
    "\n",
    "# Example texts to classify\n",
    "examples = [\n",
    "    \"NASA launches a new research satellite to study climate change effects.\",\n",
    "    \"The Barcelona lost the last Soccer Match Cricket.\"\n",
    "]\n",
    "\n",
    "for text in examples:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    pred_label_id = int(logits.argmax(dim=-1))\n",
    "    print(f\"Text: {text}\\n  Predicted label: {label_names[pred_label_id]}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c81654-fa2b-435e-b4d2-4a49aea08b70",
   "metadata": {},
   "source": [
    "### Load the saved Model again and do a fresh Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40433b5a-ae4c-4427-9661-d3f5d9935934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at TinyLlama/TinyLlama-1.1B-Chat-v1.0 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: NASA launches a new research satellite to study climate change effects.\n",
      "  Predicted label: Sci/Tech\n",
      "\n",
      "Text: The Barcelona lost the last Soccer Match Cricket.\n",
      "  Predicted label: Sports\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# 1) Load your tokenizer from the folder you saved\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./final_model\")\n",
    "\n",
    "# 2) Load the original base model with the **correct** num_labels\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",  # same base model you used for training\n",
    "    num_labels=4,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 3) Attach your LoRA adapters from disk\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"./final_model\",    # this folder has the adapter weights + PEFT config\n",
    "    inference_mode=True # optional: disables gradients\n",
    ")\n",
    "\n",
    "# Now `model` has your trained head of size 4 and your LoRA adapters.\n",
    "# Put model in evaluation mode and move to CPU (or GPU) for inference\n",
    "model.eval()\n",
    "model.to(\"cuda\")\n",
    "\n",
    "# Example texts to classify\n",
    "examples = [\n",
    "    \"NASA launches a new research satellite to study climate change effects.\",\n",
    "    \"The Barcelona lost the last Soccer Match Cricket.\"\n",
    "]\n",
    "\n",
    "for text in examples:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    pred_label_id = int(logits.argmax(dim=-1))\n",
    "    print(f\"Text: {text}\\n  Predicted label: {label_names[pred_label_id]}\\n\")\n",
    "\n",
    "# 1) Merge adapters into base weights and unload the PEFT wrapper\n",
    "merged = model.merge_and_unload()  \n",
    "merged.save_pretrained(\"./merged_model\")\n",
    "tokenizer.save_pretrained(\"./merged_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d3ab65-8454-43bf-9d6b-36b13557bc5c",
   "metadata": {},
   "source": [
    "## Final Test (You can run this code Directly after pip installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "920008f2-e204-49b4-b61b-dd3ffada44b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_3', 'score': 0.9999626874923706}, {'label': 'LABEL_1', 'score': 0.9979887008666992}]\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "\n",
    "\n",
    "# 2) Now reload as a normal SequenceClassification model\n",
    "clf = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"./merged_model\",\n",
    "    tokenizer=\"./merged_model\",\n",
    "    device=0\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    \"NASA launches a new research satellite to study climate change effects.\",\n",
    "    \"Barcelona lost the last soccer match 2-1.\"\n",
    "]\n",
    "print(clf(examples))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
